{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9292c617",
   "metadata": {},
   "source": [
    "I. Indeed et les stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97748b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-jobspy\n",
      "  Downloading python_jobspy-1.1.80-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting NUMPY==1.26.3 (from python-jobspy)\n",
      "  Downloading numpy-1.26.3-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-jobspy) (4.12.3)\n",
      "Collecting markdownify<0.14.0,>=0.13.1 (from python-jobspy)\n",
      "  Downloading markdownify-0.13.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-jobspy) (2.2.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.3.0 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from python-jobspy) (2.11.2)\n",
      "Collecting regex<2025.0.0,>=2024.4.28 (from python-jobspy)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from python-jobspy) (2.32.3)\n",
      "Collecting tls-client<2.0.0,>=1.0.1 (from python-jobspy)\n",
      "  Downloading tls_client-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (2.5)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdownify<0.14.0,>=0.13.1->python-jobspy) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.1.0->python-jobspy) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\grego\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2025.1.31)\n",
      "Downloading python_jobspy-1.1.80-py3-none-any.whl (47 kB)\n",
      "Downloading numpy-1.26.3-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading markdownify-0.13.1-py3-none-any.whl (10 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading tls_client-1.0.1-py3-none-any.whl (41.3 MB)\n",
      "Installing collected packages: tls-client, regex, NUMPY, markdownify, python-jobspy\n",
      "Successfully installed NUMPY-1.26.3 markdownify-0.13.1 python-jobspy-1.1.80 regex-2024.11.6 tls-client-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  WARNING: The script markdownify.exe is installed in 'C:\\Users\\grego\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, which is not installed.\n",
      "streamlit 1.32.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U python-jobspy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e7e66",
   "metadata": {},
   "source": [
    "1. Les stages en Angleterre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62c9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 jobs\n"
     ]
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuration de la recherche\n",
    "search_term = \"data scientist trainee\"\n",
    "results_wanted = 150\n",
    "\n",
    "# Ajouter un délai pour réduire les risques de blocage\n",
    "time.sleep(5)\n",
    "\n",
    "# Fonction principale\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\"],\n",
    "    search_term=search_term,\n",
    "    results_wanted=results_wanted,\n",
    "    country_indeed=\"UK\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "if not jobs.empty:\n",
    "    jobs.to_csv(\"indeed_stageUK.csv\", index=False)\n",
    "else:\n",
    "    print(\"Aucune offre trouvée ou accès bloqué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039d08a",
   "metadata": {},
   "source": [
    "2. Les stages en Allemagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb47641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 jobs\n"
     ]
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuration de la recherche\n",
    "search_term = \"data scientist trainee\"\n",
    "results_wanted = 150\n",
    "\n",
    "# Ajouter un délai pour réduire les risques de blocage\n",
    "time.sleep(5)\n",
    "\n",
    "# Lancement du scraping\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\"],\n",
    "    search_term=search_term,\n",
    "    results_wanted=results_wanted,\n",
    "    country_indeed=\"Germany\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "if not jobs.empty:\n",
    "    jobs.to_csv(\"indeed_stageGermany.csv\", index=False)\n",
    "else:\n",
    "    print(\"Aucune offre trouvée ou accès bloqué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc68fbd",
   "metadata": {},
   "source": [
    "3. L'Allemagne : allemand vs (et) l'anglais ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd85f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 jobs\n"
     ]
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuration de la recherche\n",
    "search_term = \"data scientist Praktikum\"\n",
    "results_wanted = 150\n",
    "\n",
    "# Ajouter un délai pour réduire les risques de blocage\n",
    "time.sleep(5)\n",
    "\n",
    "# Lancement du scraping\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\"],\n",
    "    search_term=search_term,\n",
    "    results_wanted=results_wanted,\n",
    "    country_indeed=\"Germany\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "if not jobs.empty:\n",
    "    jobs.to_csv(\"indeed_stageGermanyPRAKTIKUM.csv\", index=False)\n",
    "else:\n",
    "    print(\"Aucune offre trouvée ou accès bloqué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006aeae",
   "metadata": {},
   "source": [
    "4. USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3651079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 jobs\n"
     ]
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuration de la recherche\n",
    "search_term = \"data scientist Trainee\"\n",
    "results_wanted = 150\n",
    "\n",
    "# Ajouter un délai pour réduire les risques de blocage\n",
    "time.sleep(5)\n",
    "\n",
    "# Lancement du scraping\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\"],\n",
    "    search_term=search_term,\n",
    "    results_wanted=results_wanted,\n",
    "    country_indeed=\"USA\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "if not jobs.empty:\n",
    "    jobs.to_csv(\"indeed_stageUSA.csv\", index=False)\n",
    "else:\n",
    "    print(\"Aucune offre trouvée ou accès bloqué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11752b",
   "metadata": {},
   "source": [
    "II. Retourner aux sources : la France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60350c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 119 jobs\n"
     ]
    }
   ],
   "source": [
    "from jobspy import scrape_jobs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuration de la recherche\n",
    "search_term = \"data scientist stage\"\n",
    "results_wanted = 150\n",
    "\n",
    "# Ajouter un délai pour réduire les risques de blocage\n",
    "time.sleep(5)\n",
    "\n",
    "# Lancement du scraping\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\"],\n",
    "    search_term=search_term,\n",
    "    results_wanted=results_wanted,\n",
    "    country_indeed=\"France\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "\n",
    "# Sauvegarde des résultats\n",
    "if not jobs.empty:\n",
    "    jobs.to_csv(\"indeed_stageFrance.csv\", index=False)\n",
    "else:\n",
    "    print(\"Aucune offre trouvée ou accès bloqué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca532c",
   "metadata": {},
   "source": [
    "III. Google et stage en France : (sauf indeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 416 jobs\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from jobspy import scrape_jobs\n",
    "\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"linkedin\", \"zip_recruiter\", \"glassdoor\", \"google\", \"bayt\", \"naukri\"],\n",
    "    search_term=\"data stage\",\n",
    "    google_search_term=\"data stage\",\n",
    "    location=\"France\",\n",
    "    results_wanted=200,\n",
    "    hours_old=72,\n",
    "    country_indeed='France',\n",
    ")\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "print(jobs.head())\n",
    "jobs.to_csv(\"stagesFrance5SITES(Google).csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674e8fd",
   "metadata": {},
   "source": [
    "IV. Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage de données : suppression des doublons et export JSON\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_path = '.'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"\\nFichier en cours : {file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        before = len(df)\n",
    "        df_clean = df.drop_duplicates(subset=[\"title\", \"company\"], keep=\"first\")\n",
    "        after = len(df_clean)\n",
    "        print(f\"{before - after} doublons supprimés\")\n",
    "        output_file = file.replace('.csv', '_cleaned.json')\n",
    "        df_clean.to_json(output_file, orient=\"records\", lines=True, force_ascii=False)\n",
    "        print(f\"Export JSON : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {file} : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les None : Nettoyage automatique\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Répertoire actuel\n",
    "folder_path = '.'\n",
    "seuil = 0.60\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"\\nTraitement de : {file}\")\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    missing_ratio = df.isna().mean()\n",
    "    cols_to_drop = missing_ratio[missing_ratio > seuil].index.tolist()\n",
    "    if cols_to_drop:\n",
    "        print(f\"Colonnes supprimées (>60% NaN) : {cols_to_drop}\")\n",
    "    else:\n",
    "        print(\"Aucune colonne supprimée\")\n",
    "    df_cleaned = df.drop(columns=cols_to_drop)\n",
    "    cleaned_filename = file.replace('.csv', '_cleaned.csv')\n",
    "    df_cleaned.to_csv(os.path.join(folder_path, cleaned_filename), index=False)\n",
    "    print(f\"Sauvegardé sous : {cleaned_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
